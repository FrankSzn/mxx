{
  "name": "mxx",
  "tagline": "C++11 Message Passing",
  "body": "mxx\r\n===\r\n[![Build Status](https://img.shields.io/travis/patflick/mxx.svg)](https://travis-ci.org/patflick/mxx)\r\n[![Build Status](https://travis-ci.org/patflick/mxx.svg?branch=master)](https://travis-ci.org/patflick/mxx)\r\n[![codecov](https://codecov.io/gh/patflick/mxx/branch/master/graph/badge.svg)](https://codecov.io/gh/patflick/mxx)\r\n[![Apache 2.0 License](https://img.shields.io/badge/license-Apache%20v2.0-blue.svg)](LICENSE)\r\n\r\n`mxx` is a `C++`/`C++11` template library for `MPI`. The main goal of this\r\nlibrary is to provide two things:\r\n\r\n1. Simplified, efficient, and type-safe C++11 bindings to common MPI operations.\r\n2. A collection of scalable, high-performance standard algorithms for parallel\r\n   distributed memory architectures, such as sorting.\r\n\r\nAs such, `mxx` is targeting use in rapid `C++` and `MPI` algorithm\r\ndevelopment, prototyping, and deployment.\r\n\r\n### Features\r\n\r\n\r\n-  All functions are templated by type. All `MPI_Datatype` are deducted\r\n   from the C++ type given to the function.\r\n-  Custom reduction operations as lambdas, `std::function`, functor, or function\r\n   pointer.\r\n-  Send/Receive and Collective operations take `size_t` sized input and\r\n   automatically handle sizes larger than `INT_MAX`.\r\n-  Plenty of convenience functions and overloads for common MPI operations with\r\n   sane defaults (e.g., super easy collectives: `std::vector<size_t> allsizes =\r\n   mxx::allgather(local_size)`).\r\n-  Automatic type mapping of all built-in (`int`, `double`, etc) and other\r\n   C++ types such as `std::tuple`, `std::pair`, and `std::array`.\r\n-  Non-blocking operations return a `mxx::future<T>` object, similar to\r\n   `std::future`.\r\n-  Google Test based `MPI` unit testing framework\r\n-  Parallel sorting with similar API than `std::sort` (`mxx::sort`)\r\n\r\n### Planned / TODO\r\n\r\n- [ ] Parallel random number engines (for use with `C++11` standard library distributions)\r\n- [ ] More parallel (standard) algorithms\r\n- [ ] Wrappers for non-blocking collectives\r\n- [ ] serialization/de-serialization of non contiguous data types (maybe)\r\n- [x] ~~macros for easy datatype creation and handling for custom/own structs and classes~~\r\n- [ ] Implementing and tuning different sorting algorithms\r\n- [ ] Communicator classes for different topologies\r\n- [x] ~~`mxx::env` similar to `boost::mpi::env` for wrapping `MPI_Init` and `MPI_Finalize`~~\r\n- [ ] full-code and intro documentations\r\n- [ ] Increase test coverage:\r\n![codecov.io](http://codecov.io/github/patflick/mxx/branch.svg?branch=master)\r\n\r\n### Status\r\n\r\nCurrently `mxx` is a small personal project at early stages, with lots of\r\nchanges still going on. However, feel free to contribute.\r\n\r\n### Examples\r\n\r\n#### Collective Operations\r\n\r\nThis example shows the main features of `mxx`'s wrappers for MPI collective\r\noperations:\r\n\r\n-   `MPI_Datatype` deduction according to the template type\r\n-   Handling of message sizes larger than `INT_MAX` (everything is `size_t`\r\n    enabled)\r\n-   Receive sizes do not have to be specified\r\n-   convenience functions for `std::vector`, both for sending and receiving\r\n\r\n```c++\r\n    // local numbers, can be different size on each process\r\n    std::vector<size_t> local_numbers = ...;\r\n    // allgather the local numbers, easy as pie:\r\n    std::vector<size_t> all_numbers = mxx::allgatherv(local_numbers, MPI_COMM_WORLD);\r\n```\r\n\r\n#### Reductions\r\n\r\nThe following example showcases the C++11 interface to reductions:\r\n\r\n```c++\r\n    #include <mxx/reduction.hpp>\r\n\r\n    // ...\r\n    // lets take some pairs and find the one with the max second element\r\n    std::pair<int, double> v = ...;\r\n    std::pair<int, double> min_pair = mxx::allreduce(v,\r\n                           [](const std::pair<int, double>& x,\r\n                              const std::pair<int, double>& y){\r\n                               return x.second > y.second ? x : y;\r\n                           });\r\n```\r\nWhat happens here, is that the C++ types are automatically matched to the\r\nappropriate `MPI_Datatype` (struct of `MPI_INT` and `MPI_DOUBLE`),\r\nthen a custom reduction operator (`MPI_Op`) is created from\r\nthe given lambda, and finally `MPI_Allreduce` called for the given parameters.\r\n\r\n#### Sorting\r\n\r\nConsider a simple example, where you might want to sort tuples `(int key,double\r\nx, double y)` by key `key` in parallel using `MPI`. Doing so in pure C/MPI\r\nrequires quite a lot of coding (~100 lines), debugging, and frustration. Thanks\r\nto `mxx` and `C++11`, this becomes as easy as:\r\n\r\n```c++\r\n    typedef std::tuple<int, double, double> tuple_type;\r\n    std::vector<tuple_type> data(local_size);\r\n    // define a comparator for the tuple\r\n    auto cmp = [](const tuple_type& x, const tuple_type& y) {\r\n                   return std::get<0>(x) < std::get<0>(y); }\r\n\r\n    // fill the vector ...\r\n\r\n    // call mxx::sort to do all the heavy lifting:\r\n    mxx::sort(data.begin(), data.end(), cmp, MPI_COMM_WORLD);\r\n```\r\n\r\nIn the background, `mxx` performs many things, including (but not limited to):\r\n\r\n- mapping the `std::tuple` to a MPI type by creating the appropriate MPI\r\n  datatype (i.e., `MPI_Type_struct`).\r\n- distributing the data if not yet done so\r\n- calling `std::sort` as a local base case, in case the communicator consists of a\r\n  single processor, `mxx::sort` will fall-back to `std::sort`\r\n- in case the data size exceeds the infamous `MPI` size limit of `MAX_INT`,\r\n  `mxx` will not fail, but continue to work as expected\r\n- redistributing the data so that it has the same distribution as given in the\r\n  input to `mxx::sort`\r\n\r\n\r\n### Alternatives?\r\n\r\nTo our knowledge, there are two noteworthy, similar open libraries available.\r\n\r\n1. [**boost::mpi**](https://github.com/boostorg/mpi) offers C++ bindings for a\r\n   large number of MPI functions. As such it corresponds to our main goal *1*.\r\n   Major drawbacks of using *boost::mpi* are the unnecessary overhead of\r\n   *boost::serialization* (especially in terms of memory overhead).\r\n   *boost::mpi* also doesn't support large message sizes (> `INT_MAX`), and\r\n   the custom reduction operator implementation is rather limited.\r\n2. [**mpp**](https://github.com/motonacciu/mpp) offers low-overhead C++ bindings\r\n   for MPI point-to-point communication primitives. As such, this solutions\r\n   shows better performance than *boost::mpi*, but was never continued beyond\r\n   point-to-point communication.\r\n\r\n### Authors\r\n\r\n- Patrick Flick\r\n\r\n## Installation\r\n\r\nSince this is a header only library, simply copy and paste the `mxx` folder into\r\nyour project, and you'll be all set.\r\n\r\n### Dependencies\r\n\r\n`mxx` requires a `C++11` compatible compiler.\r\n`mxx` currently works with `MPI-2` and `MPI-3`.\r\nHowever, some collective operations and sorting will work on data sizes `>= 2 GB` only with `MPI-3`.\r\n\r\n### Compiling\r\n\r\nNot necessary. This is a header only library. There is nothing to compile.\r\n\r\n#### Building tests\r\n\r\nThe tests can be compiled using `cmake`:\r\n\r\n```sh\r\nmkdir build && cd build\r\ncmake ../ && make\r\n```\r\n\r\nRunning the tests (with however many processes you want).\r\n```\r\nmpirun -np 13 ./bin/test-all\r\n```\r\n\r\n\r\n## Licensing\r\n\r\nOur code is licensed under the\r\n**Apache License 2.0** (see [`LICENSE`](LICENSE)).\r\nThe licensing does not apply to the `ext` folder, which contains external\r\ndependencies which are under their own licensing terms.\r\n",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}